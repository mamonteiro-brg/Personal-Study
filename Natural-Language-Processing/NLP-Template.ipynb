{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mamonteiro-brg/Personal-Study/blob/master/Natural-Language-Processing/NLP-Template.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DgIaNzos-YeR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2y7NIff-ZF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Unit 1: Working with Text data**\n",
        "\n",
        "\n",
        "Unstructured data and vectorization\n",
        "\n",
        "How to apply machine learning to unstructured data.\n",
        "\n",
        "> What is vectorization and why itâ€™s useful.\n",
        "\n",
        "\n",
        "  \n",
        "**feature extraction**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> How to vectorize text to extract features.\n",
        "\n",
        "> Counting words occurrences.\n",
        "\n",
        "> What is Bag-of-Words approach.\n",
        "\n",
        "> What is n-grams.\n",
        "\n",
        "\t\n",
        "\t\n",
        "\n",
        "\n",
        "Feature selection\n",
        "\tNot all words are useful: stop-words, stemming.\n",
        "  \n",
        "Feature weighting\n",
        "\tBeyond counting: TF, IDF.\n",
        "  \n",
        "Classifying with Naive Bayes\n",
        ">Computing the probability of class C given the evidence.\n",
        ">Maximum a Posteriori Probability and independence assumption.\n",
        ">Numerical problems.\n",
        "\n",
        "**Unit 2: Text data in practice**\n",
        "\n",
        "Data exploration\n",
        "Loading News Aggregator dataset and playing around\n",
        "\n",
        ">NLTK, Scikit\n",
        "\t>Introducing NLTK and Scikit for text\n",
        "\n",
        ">Stemming with NLTK\n",
        "\t>Show how to do stemming with NLTK\n",
        "\n",
        ">Feature extraction with Scikit\n",
        "\t>Extract vector representation using CountVectorizer and TfIdfVectorizer\n",
        "\n",
        ">Feature importances\n",
        "\t>Which words are most important for discriminate classes?\n",
        "\n",
        ">Classifying text\n",
        "\tUse MultinomialNB and LinearSVC to classify vectorized text\n",
        "\n",
        "Pipelines\n",
        ">Putting all together in a Pipeline\n",
        "  \n",
        "\n",
        "**Unit 3: Advanced topics**\n",
        "\n",
        "Word2Vec\n",
        ">Deep learning for text vectorization, show how it works\n",
        "\n",
        "Recurrent Networks\n",
        ">Classifying text with LSTM\n",
        "\n",
        "Clustering\n",
        ">Dimensionality reduction\n"
      ]
    }
  ]
}