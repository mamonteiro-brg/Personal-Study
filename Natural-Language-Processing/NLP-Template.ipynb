{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mamonteiro-brg/Personal-Study/blob/master/Natural-Language-Processing/NLP-Template.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DgIaNzos-YeR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2y7NIff-ZF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unit 1: Working with Text data\n",
        "\n",
        "Unstructured data and vectorization\n",
        "\n",
        "How to apply machine learning to unstructured data.\n",
        "\tWhat is vectorization and why itâ€™s useful.\n",
        "  \n",
        "Feature extraction\n",
        "\tHow to vectorize text to extract features.\n",
        "\tCounting words occurrences.\n",
        "\tWhat is Bag-of-Words approach.\n",
        "\tWhat is n-grams.\n",
        "  \n",
        "Feature selection\n",
        "\tNot all words are useful: stop-words, stemming.\n",
        "  \n",
        "Feature weighting\n",
        "\tBeyond counting: TF, IDF.\n",
        "  \n",
        "Classifying with Naive Bayes\n",
        "Computing the probability of class C given the evidence.\n",
        "Maximum a Posteriori Probability and independence assumption.\n",
        "Numerical problems.\n",
        "\n",
        "Unit 2: Text data in practice\n",
        "Data exploration\n",
        "Loading News Aggregator dataset and playing around\n",
        "NLTK, Scikit\n",
        "\tIntroducing NLTK and Scikit for text\n",
        "\n",
        "Stemming with NLTK\n",
        "\tShow how to do stemming with NLTK\n",
        "\n",
        "Feature extraction with Scikit\n",
        "\tExtract vector representation using CountVectorizer and TfIdfVectorizer\n",
        "\n",
        "Feature importances\n",
        "\tWhich words are most important for discriminate classes?\n",
        "\n",
        "Classifying text\n",
        "\tUse MultinomialNB and LinearSVC to classify vectorized text\n",
        "\n",
        "Pipelines\n",
        "\tPutting all together in a Pipeline\n",
        "\n",
        "Unit 3: Advanced topics\n",
        "Word2Vec\n",
        "\tDeep learning for text vectorization, show how it works\n",
        "\n",
        "Recurrent Networks\n",
        "\tClassifying text with LSTM\n",
        "\n",
        "Clustering\n",
        "Dimensionality reduction\n"
      ]
    }
  ]
}