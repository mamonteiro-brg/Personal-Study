{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mamonteiro-brg/Personal-Study/blob/master/Natural-Language-Processing/NLP-Template.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DgIaNzos-YeR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2y7NIff-ZF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Unit 1: Working with Text data**\n",
        "\n",
        "\n",
        "\n",
        ">**Unstructured data and vectorization**\n",
        "\n",
        ">>How to apply machine learning to unstructured data.\n",
        "\n",
        ">> What is vectorization and why itâ€™s useful.\n",
        "\n",
        "  \n",
        ">**feature extraction**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">> How to vectorize text to extract features.\n",
        "\n",
        ">> Counting words occurrences.\n",
        "\n",
        ">> What is Bag-of-Words approach.\n",
        "\n",
        ">> What is n-grams.\n",
        "\n",
        "\t\n",
        "\t\n",
        "\n",
        "\n",
        ">**Feature selection**\n",
        ">>Not all words are useful: stop-words, stemming.\n",
        "  \n",
        ">**Feature weighting**\n",
        "\n",
        ">>Beyond counting: TF, IDF.\n",
        "  \n",
        ">**Classifying with Naive Bayes**\n",
        "\n",
        ">>Computing the probability of class C given the evidence.\n",
        ">>Maximum a Posteriori Probability and independence assumption.\n",
        ">>Numerical problems.\n",
        "\n",
        "**Unit 2: Text data in practice**\n",
        "\n",
        "\n",
        ">**Data exploration**\n",
        "\n",
        ">>Loading News Aggregator dataset and playing around\n",
        "\n",
        ">**NLTK, Scikit**\n",
        "\n",
        ">>Introducing NLTK and Scikit for text\n",
        "\n",
        ">**Stemming with NLTK**\n",
        "\n",
        ">>Show how to do stemming with NLTK\n",
        "\n",
        ">**Feature extraction with Scikit**\n",
        "\n",
        ">>Extract vector representation using CountVectorizer and TfIdfVectorizer\n",
        "\n",
        ">**Feature importances**\n",
        "\n",
        ">>Which words are most important for discriminate classes?\n",
        "\n",
        ">**Classifying text**\n",
        "\n",
        ">>Use MultinomialNB and LinearSVC to classify vectorized text\n",
        "\n",
        ">**Pipelines**\n",
        "\n",
        ">>Putting all together in a Pipeline\n",
        "  \n",
        "\n",
        "**Unit 3: Advanced topics**\n",
        "\n",
        ">**Word2Vec**\n",
        ">>Deep learning for text vectorization, show how it works\n",
        "\n",
        ">**Recurrent Networks**\n",
        ">>Classifying text with LSTM\n",
        "\n",
        ">**Clustering**\n",
        ">>Dimensionality reduction\n"
      ]
    }
  ]
}